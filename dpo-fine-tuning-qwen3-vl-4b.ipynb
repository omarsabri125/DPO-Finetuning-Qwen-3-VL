{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü¶ô DPO Fine-Tuning with LlamaFactory ‚Äî Qwen3-VL-4B Vision-Language Model\n",
    "\n",
    "---\n",
    "\n",
    "## üìñ Overview\n",
    "\n",
    "This notebook walks through a complete **Direct Preference Optimization (DPO)** fine-tuning pipeline for a **Vision-Language Model (VLM)** using the **LlamaFactory** framework.\n",
    "\n",
    "### What is DPO?\n",
    "**Direct Preference Optimization (DPO)** is a fine-tuning technique that teaches a language model to prefer certain responses over others, without requiring a separate reward model. Instead of reinforcement learning, it directly optimizes the model using pairs of:\n",
    "- ‚úÖ **Chosen** response ‚Äî the preferred, higher-quality answer\n",
    "- ‚ùå **Rejected** response ‚Äî the non-preferred, lower-quality answer\n",
    "\n",
    "### What is LlamaFactory?\n",
    "**LlamaFactory** is an open-source, unified fine-tuning framework that supports:\n",
    "- Multiple training methods: SFT, DPO, ORPO, KTO, PPO, and more\n",
    "- Efficient adapters: LoRA, QLoRA, full fine-tuning\n",
    "- Many model families: LLaMA, Qwen, Mistral, Gemma, and more\n",
    "- Multimodal models (images, audio, video)\n",
    "\n",
    "### Model Used\n",
    "We fine-tune **Qwen3-VL-4B-Instruct**, a 4-billion parameter vision-language model from Alibaba's Qwen3 family, capable of understanding both text and images.\n",
    "\n",
    "### Dataset\n",
    "We use the **`helehan/topic-overwrite`** dataset from HuggingFace, which contains image-question pairs with chosen and rejected answers ‚Äî perfect for DPO training.\n",
    "\n",
    "---\n",
    "\n",
    "## üó∫Ô∏è Pipeline Overview\n",
    "\n",
    "```\n",
    "1. Load Dataset          ‚Üí  HuggingFace dataset with images + chosen/rejected answers\n",
    "2. Process Images        ‚Üí  Save as JPEG, map paths back to DataFrame\n",
    "3. Format for DPO        ‚Üí  Convert to LlamaFactory's ShareGPT DPO format\n",
    "4. Download Extra Data   ‚Üí  Pull pre-prepared files from Google Drive\n",
    "5. Fix Image Paths       ‚Üí  Remap paths from Colab ‚Üí Kaggle\n",
    "6. Install Dependencies  ‚Üí  torch, transformers, LlamaFactory\n",
    "7. Register Datasets     ‚Üí  Add custom datasets to LlamaFactory registry\n",
    "8. Write YAML Config     ‚Üí  Define all training hyperparameters\n",
    "9. Run Training          ‚Üí  Launch DPO training via CLI\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***EDA***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Import Libraries & Load Dataset\n",
    "\n",
    "### What this cell does:\n",
    "This is the foundation cell. We import all the Python libraries we need throughout the notebook and load the raw dataset from HuggingFace.\n",
    "\n",
    "### Libraries explained:\n",
    "\n",
    "| Library | Purpose |\n",
    "|---|---|\n",
    "| `os`, `glob` | File system operations ‚Äî navigating directories, finding files by pattern |\n",
    "| `base64` | Encoding binary data (images) into text ‚Äî sometimes needed for API calls |\n",
    "| `tqdm` | Progress bars ‚Äî shows how long loops will take |\n",
    "| `datasets` | HuggingFace library for loading and processing datasets |\n",
    "| `json` | Reading and writing JSON files (our data format) |\n",
    "| `pandas` | DataFrame operations ‚Äî filtering, indexing, splitting data |\n",
    "| `sklearn` | `train_test_split` for splitting data into train/validation sets |\n",
    "| `PIL (Pillow)` | Image processing ‚Äî opening, converting, saving images |\n",
    "| `matplotlib` | Plotting ‚Äî visualizing training curves and sample images |\n",
    "| `gdown` | Downloading files directly from Google Drive by file ID |\n",
    "\n",
    "### Dataset:\n",
    "The `helehan/topic-overwrite` dataset is a vision DPO dataset. Each sample contains:\n",
    "- An **image**\n",
    "- A **question** about that image\n",
    "- A **chosen** (good) answer\n",
    "- A **rejected** (bad) answer\n",
    "\n",
    "We load only the `train` split and immediately convert it to a pandas DataFrame for easier manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gdown'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgdown\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gdown'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import base64\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\n",
    "    \"helehan/topic-overwrite\", split=\"train\"\n",
    ")\n",
    "df = dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üñºÔ∏è Process & Save Images as JPEG\n",
    "\n",
    "### Why do we need this cell?\n",
    "The images in HuggingFace datasets are stored as in-memory PIL Image objects. LlamaFactory needs images saved as **actual files on disk** with known paths. This cell:\n",
    "1. Creates a local folder `imagesdpo/` to store images\n",
    "2. Converts every image to RGB (some images may be RGBA, grayscale, or palette mode)\n",
    "3. Saves each image as a compressed JPEG\n",
    "4. Builds a sorted list of image paths\n",
    "5. Maps those paths back to the DataFrame\n",
    "\n",
    "### Key decisions:\n",
    "- **RGB conversion**: JPEG format does not support transparency (alpha channel). Images with mode `RGBA`, `P` (palette), or `L` (grayscale) must be converted to `RGB` first or saving will fail.\n",
    "- **JPEG quality=85**: A good balance between file size and visual quality. Lower = smaller files but more compression artifacts.\n",
    "- **optimize=True**: Applies Huffman coding optimization ‚Äî slightly slower saving but smaller file size.\n",
    "- **Filename format**: `{original_name}_{index}.jpg` ‚Äî the index at the end ensures unique filenames even if original names collide.\n",
    "- **Sorted by index**: After glob (which returns files in arbitrary OS order), we sort by the numeric index in the filename so image order matches DataFrame row order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "os.makedirs(\"imagesdpo\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for i, sample in tqdm(enumerate(dataset)):\n",
    "  imag = sample['image']\n",
    "  if imag.mode != 'RGB':\n",
    "    imag = imag.convert('RGB')\n",
    "  file_name = f\"{os.path.splitext(os.path.basename(sample['image_path']))[0]}_{i}.jpg\"\n",
    "  path = f\"imagesdpo/{file_name}\"\n",
    "  imag.save(path,'JPEG', quality=85, optimize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "images = glob(\"imagesdpo/*.jpg\")\n",
    "images = sorted(images,\n",
    "               key=lambda x: int(os.path.splitext(os.path.basename(x))[0].split(\"_\")[-1]))\n",
    "df['image_edited'] = images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "image = Image.open(images[0])\n",
    "plt.imshow(image)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÇÔ∏è Train / Validation Split\n",
    "\n",
    "### Why split the data?\n",
    "In any machine learning workflow, we split data into:\n",
    "- **Training set (80%)**: The model learns from this data\n",
    "- **Validation set (20%)**: We evaluate the model on this data to detect overfitting and track performance during training\n",
    "\n",
    "### Parameters explained:\n",
    "- **`test_size=0.2`**: 20% of data goes to validation, 80% stays for training\n",
    "- **`shuffle=True`**: Randomly shuffles the data before splitting ‚Äî important so that the model doesn't learn from a biased ordering\n",
    "- **`random_state=42`**: Fixes the random seed so results are **reproducible** ‚Äî running the same code always gives the same split. (42 is a convention in ML, from *The Hitchhiker's Guide to the Galaxy*)\n",
    "\n",
    "### Important note:\n",
    "The split happens at the **DataFrame level**, meaning both `train_df` and `test_df` retain all columns including `image_edited`, `question`, `chosen`, and `rejected`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.2, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Format Data into LlamaFactory DPO Format\n",
    "\n",
    "### Why do we need a special format?\n",
    "LlamaFactory expects data in a specific **ShareGPT-style JSON structure** for DPO training. Raw data from HuggingFace doesn't match this structure, so we convert it.\n",
    "\n",
    "### The DPO record structure:\n",
    "```json\n",
    "{\n",
    "  \"conversations\": [\n",
    "    {\"from\": \"human\", \"value\": \"<image>What is shown in this image?\"}\n",
    "  ],\n",
    "  \"chosen\":   {\"from\": \"gpt\", \"value\": \"A detailed correct answer...\"},\n",
    "  \"rejected\": {\"from\": \"gpt\", \"value\": \"A vague or wrong answer...\"},\n",
    "  \"images\":   [\"imagesdpo/photo_42.jpg\"]\n",
    "}\n",
    "```\n",
    "\n",
    "### Key format details:\n",
    "\n",
    "| Field | Description |\n",
    "|---|---|\n",
    "| `conversations` | List of conversation turns. For DPO we only need the **user turn** (the question) |\n",
    "| `\"from\": \"human\"` | Marks this turn as coming from the user |\n",
    "| `\"<image>\"` | Special token that tells the model where the image appears in the text |\n",
    "| `chosen` | The preferred response ‚Äî the answer DPO wants the model to **favor** |\n",
    "| `rejected` | The non-preferred response ‚Äî the answer DPO wants the model to **avoid** |\n",
    "| `images` | List of image file paths. Must match the number of `<image>` tokens in conversations |\n",
    "\n",
    "### Why `<image>` before the question?\n",
    "Placing `<image>` before the text is the standard convention for Qwen-VL models ‚Äî it tells the model to process the visual context before reading the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def format_fine_tuning(df):\n",
    "  formatted_data = []\n",
    "  for i in range(len(df)):\n",
    "    task_dpo_record = {\n",
    "    \"conversations\": [\n",
    "        {\n",
    "          \"from\": \"human\",\n",
    "          \"value\": \"<image>\"+df.iloc[i]['question'],\n",
    "        },\n",
    "      ],\n",
    "    \"chosen\": {\n",
    "      \"from\": \"gpt\",\n",
    "      \"value\": df.iloc[i]['chosen']\n",
    "    },\n",
    "    \"rejected\": {\n",
    "      \"from\": \"gpt\",\n",
    "      \"value\": df.iloc[i]['rejected']\n",
    "    },\n",
    "    \"images\": [\n",
    "        df.iloc[i]['image_edited']\n",
    "      ]\n",
    "    }\n",
    "    formatted_data.append(task_dpo_record)\n",
    "  return formatted_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dpo = format_fine_tuning(train_df)\n",
    "test_dpo = format_fine_tuning(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Save Formatted JSON Files\n",
    "\n",
    "### Why save to JSON?\n",
    "LlamaFactory reads training data from **local JSON files**. We need to serialize our Python list of dictionaries into JSON format and save them at known paths.\n",
    "\n",
    "### JSON writing parameters explained:\n",
    "- **`ensure_ascii=False`**: Allows non-ASCII characters (Arabic, Chinese, accented letters, emoji) to be saved as-is instead of being escaped as `\\uXXXX` sequences. This is important if questions/answers contain non-English text.\n",
    "- **`default=str`**: If any Python object can't be serialized to JSON (e.g., a numpy int64 or a Path object), convert it to a string instead of crashing. A safety net.\n",
    "\n",
    "### File structure created:\n",
    "```\n",
    "imagesdpo/\n",
    "‚îî‚îÄ‚îÄ datasets/\n",
    "    ‚îî‚îÄ‚îÄ llamafactory-dpo-finetune-data/\n",
    "        ‚îú‚îÄ‚îÄ train-v1.json   ‚Üê 80% of data for training\n",
    "        ‚îî‚îÄ‚îÄ val-v1.json     ‚Üê 20% of data for evaluation\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "os.makedirs(\n",
    "    os.path.join(\"imagesdpo\", \"datasets\", \"llamafactory-dpo-finetune-data\"), exist_ok=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(\"imagesdpo\", \"datasets\", \"llamafactory-dpo-finetune-data\", \"train-v1.json\") , \"w\") as dest:\n",
    "    json.dump(train_dpo, dest, ensure_ascii=False, default=str)\n",
    "\n",
    "with open(os.path.join(\"imagesdpo\", \"datasets\", \"llamafactory-dpo-finetune-data\", \"val-v1.json\") , \"w\") as dest:\n",
    "    json.dump(test_dpo, dest, ensure_ascii=False, default=str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚òÅÔ∏è Download Pre-Prepared Data from Google Drive\n",
    "\n",
    "### Why download from Google Drive?\n",
    "This project was originally developed in **Google Colab** where the dataset (images + formatted JSON files) was prepared and saved directly to **Google Drive**. When it came time to train, Colab's free GPU (**Tesla T4**) was too weak to handle a 4B vision-language model with DPO training efficiently.\n",
    "\n",
    "So the workflow was moved to **Kaggle**, which offers a free **NVIDIA P100** GPU that is significantly more powerful for training:\n",
    "\n",
    "| Platform | GPU | VRAM | Why |\n",
    "|---|---|---|---|\n",
    "| Google Colab (free) | Tesla T4 | 15GB | Optimized for inference, weaker at FP32 training |\n",
    "| Kaggle (free) | NVIDIA P100 | 16GB | Built for scientific training workloads, stronger FP32 compute |\n",
    "\n",
    "Since the data was already prepared and sitting on Google Drive, we use `gdown` to pull it directly into the Kaggle environment instead of re-running the entire data preparation pipeline from scratch.\n",
    "\n",
    "### What is gdown?\n",
    "`gdown` is a Python library for downloading files from Google Drive using their **file ID** (the random string in a Drive sharing URL).\n",
    "\n",
    "For example, in `https://drive.google.com/file/d/1SlSKfmTzigLxJHOfL2DJYKTES5Ry0394/view`, the file ID is `1SlSKfmTzigLxJHOfL2DJYKTES5Ry0394`.\n",
    "\n",
    "### Files downloaded:\n",
    "\n",
    "| File | Content |\n",
    "|---|---|\n",
    "| `pdf_images.zip` | ZIP of images extracted from PDF documents (prepared in Colab) |\n",
    "| `train-v1.json` | Pre-formatted DPO training data in ShareGPT format (prepared in Colab) |\n",
    "| `val-v1.json` | Pre-formatted DPO validation data in ShareGPT format (prepared in Colab) |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pdf_images_file_id = \"1SlSKfmTzigLxJHOfL2DJYKTES5Ry0394\"\n",
    "gdown.download(id=pdf_images_file_id, output=\"./pdf_images.zip\")\n",
    "\n",
    "\n",
    "train_file_id = \"1YPhsnMLN3D1-qTAyKW0-bMqfha8jBtnk\"\n",
    "gdown.download(id=train_file_id, output=\"./train-v1.json\")\n",
    "\n",
    "\n",
    "val_file_id = \"1QoH17Z-VAPLkoxLc9qi09kX4DnlczI2w\"\n",
    "gdown.download(id=val_file_id, output=\"./val-v1.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÇ Unzip Images & Collect Paths\n",
    "\n",
    "### What this cell does:\n",
    "1. **Unzips** the downloaded `pdf_images.zip` archive into the working directory\n",
    "2. **Collects** all extracted `.jpg` image paths using `glob`\n",
    "\n",
    "### Why unzip?\n",
    "ZIP files must be extracted before their contents can be accessed by Python or LlamaFactory. The `-q` flag makes `unzip` quiet (no verbose output), keeping the notebook clean.\n",
    "\n",
    "### The nested path problem:\n",
    "Notice the long path after unzipping:\n",
    "```\n",
    "/kaggle/working/imagesdpo/content/drive/MyDrive/RAG Techinques/pdf_images/imagesdpo/\n",
    "```\n",
    "This happens because when the ZIP was created in Google Colab, it preserved the **full absolute Colab path** inside the archive. When unzipped on Kaggle, that Colab path becomes a subdirectory. The next cell fixes the paths in the JSON files to point here correctly.\n",
    "\n",
    "### The glob pattern:\n",
    "`glob(\"path/*.jpg\")` uses a wildcard `*` to match all files ending in `.jpg` in the specified directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!unzip -q /kaggle/working/pdf_images.zip -d /kaggle/working/imagesdpo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "images = glob(\"/kaggle/working/imagesdpo/content/drive/MyDrive/RAG Techinques/pdf_images/imagesdpo/*.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "images[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "len(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Load JSON & Fix Image Paths\n",
    "\n",
    "### Why do we need to fix paths?\n",
    "The downloaded `train-v1.json` and `val-v1.json` files were created in **Google Colab** where images were stored at:\n",
    "```\n",
    "/content/drive/MyDrive/RAG Techinques/pdf_images/imagesdpo/image.jpg\n",
    "```\n",
    "\n",
    "On **Kaggle**, the same images are located at:\n",
    "```\n",
    "/kaggle/working/imagesdpo/content/drive/MyDrive/RAG Techinques/pdf_images/imagesdpo/image.jpg\n",
    "```\n",
    "\n",
    "If we don't fix the paths, LlamaFactory will look for images that **don't exist** at those locations and crash.\n",
    "\n",
    "### How we fix it:\n",
    "We use Python's `str.replace()` to swap the old Colab prefix with the new Kaggle prefix for every single image path in every record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with open(\"/kaggle/working/train-v1.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    train_data = json.load(file)\n",
    "with open(\"/kaggle/working/val-v1.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    test_data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(train_data)):\n",
    "    train_data[i]['images'][0] = train_data[i]['images'][0].replace(\"/content/drive/MyDrive/RAG Techinques/pdf_images/imagesdpo/\",\n",
    "                                                \"/kaggle/working/imagesdpo/content/drive/MyDrive/RAG Techinques/pdf_images/imagesdpo/\")\n",
    "for i in range(len(test_data)):\n",
    "    test_data[i]['images'][0] = test_data[i]['images'][0].replace(\"/content/drive/MyDrive/RAG Techinques/pdf_images/imagesdpo/\",\n",
    "                                                              \"/kaggle/working/imagesdpo/content/drive/MyDrive/RAG Techinques/pdf_images/imagesdpo/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with open(\"./train-v1-edited.json\" , \"w\") as dest:\n",
    "    json.dump(train_data, dest, ensure_ascii=False, default=str)\n",
    "\n",
    "with open(\"./val-v1-edited.json\" , \"w\") as dest:\n",
    "    json.dump(test_data, dest, ensure_ascii=False, default=str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Finetuning***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Install All Required Dependencies\n",
    "\n",
    "### Why pin specific versions?\n",
    "Machine learning libraries change rapidly and often break compatibility between versions. Pinning **exact versions** ensures:\n",
    "- **Reproducibility**: anyone running this notebook gets the same behavior\n",
    "- **Compatibility**: LlamaFactory has been tested with these specific versions\n",
    "- **Stability**: No surprise API changes from upgrading\n",
    "\n",
    "### Packages explained:\n",
    "\n",
    "| Package | Version | Role |\n",
    "|---|---|---|\n",
    "| `transformers` | 4.57.6 | Core HuggingFace library ‚Äî model loading, tokenization, training loops |\n",
    "| `optimum` | 1.26.0 | HuggingFace optimization toolkit ‚Äî enables efficient inference and training |\n",
    "| `datasets` | 4.4.0 | HuggingFace data library ‚Äî loading, streaming, processing datasets |\n",
    "| `torch` | 2.8.0 | PyTorch ‚Äî the deep learning framework powering all computations |\n",
    "| `torchvision` | 0.23 | Image transforms and vision utilities for PyTorch |\n",
    "| `torchaudio` | 2.8.0 | Audio processing for PyTorch (included for compatibility) |\n",
    "\n",
    "### Why torch 2.8.0 specifically?\n",
    "Qwen3-VL requires modern attention mechanisms (FlashAttention2, etc.) that are only available in newer PyTorch versions. 2.8.0 also has improved memory efficiency for large vision-language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install transformers==4.57.6\n",
    "!pip install optimum==1.26.0\n",
    "!pip install datasets==4.4.0\n",
    "\n",
    "!pip install torch==2.8.0\n",
    "!pip install torchvision==0.23\n",
    "!pip install torchaudio==2.8.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü¶ô Clone & Install LlamaFactory\n",
    "\n",
    "### What is LlamaFactory?\n",
    "LlamaFactory is an open-source framework that wraps HuggingFace Transformers with:\n",
    "- A simple **YAML config** interface for training (no complex Python scripts needed)\n",
    "- Built-in support for **LoRA, QLoRA, full fine-tuning**\n",
    "- Pre-built **chat templates** for dozens of models\n",
    "- Multi-modal support (vision, audio, video)\n",
    "- A **CLI tool** (`llamafactory-cli`) for one-command training\n",
    "\n",
    "### Why a specific git commit?\n",
    "```bash\n",
    "git checkout 762b480131908d37736ad9aa3f12e87f8f7e6313\n",
    "```\n",
    "This pins LlamaFactory to a **specific commit** that is known to work with our setup. The main branch of active open-source projects can change daily ‚Äî pinning a commit prevents breaking changes from affecting our training run.\n",
    "\n",
    "### `--depth 1` in git clone:\n",
    "Downloads only the **latest snapshot** of the repository without the full git history. This is much faster and uses less disk space ‚Äî ideal for Kaggle's limited storage.\n",
    "\n",
    "### `pip install -e .`:\n",
    "Installs LlamaFactory in **editable mode** ‚Äî meaning Python imports the code directly from the cloned folder. Changes to the source files are immediately reflected without reinstalling.\n",
    "\n",
    "### `requirements/metrics.txt`:\n",
    "Installs additional libraries needed for computing evaluation metrics like BLEU, ROUGE, and perplexity during the eval phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!git clone --depth 1 https://github.com/hiyouga/LlamaFactory.git\n",
    "!cd LlamaFactory && git checkout 762b480131908d37736ad9aa3f12e87f8f7e6313\n",
    "\n",
    "!cd LlamaFactory && pip install -e .\n",
    "!cd LlamaFactory && pip install -r requirements/metrics.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Register Custom Datasets in LlamaFactory\n",
    "\n",
    "### What is `dataset_info.json`?\n",
    "LlamaFactory maintains a **central registry** of all known datasets in `data/dataset_info.json`. Before you can use any dataset in training, it must be registered here.\n",
    "\n",
    "### What information goes in the registry?\n",
    "Each dataset entry tells LlamaFactory:\n",
    "- **`file_name`**: Where to find the local JSON file\n",
    "- **`formatting`**: Data format ‚Äî `\"sharegpt\"` for multi-turn conversation format, or `\"alpaca\"` for single instruction-response pairs\n",
    "- **`ranking: True`**: This is a **preference dataset** with chosen/rejected pairs (required for DPO)\n",
    "- **`columns`**: Maps our JSON field names to LlamaFactory's expected field names\n",
    "\n",
    "### Column mapping for our DPO dataset:\n",
    "\n",
    "| Our JSON field | LlamaFactory expects | Meaning |\n",
    "|---|---|---|\n",
    "| `conversations` | `messages` | The conversation history (user turns) |\n",
    "| `chosen` | `chosen` | The preferred response |\n",
    "| `rejected` | `rejected` | The non-preferred response |\n",
    "| `images` | `images` | Paths to image files |\n",
    "\n",
    "### Why write to the LlamaFactory folder?\n",
    "LlamaFactory reads `dataset_info.json` from its own `data/` directory at training time. We must write our custom entries directly into that file ‚Äî we can't point it to another location.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data_info = {\n",
    "  \"identity\": {\n",
    "    \"file_name\": \"identity.json\"\n",
    "  },\n",
    "  \"alpaca_en_demo\": {\n",
    "    \"file_name\": \"alpaca_en_demo.json\"\n",
    "  },\n",
    "  \"alpaca_zh_demo\": {\n",
    "    \"file_name\": \"alpaca_zh_demo.json\"\n",
    "  },\n",
    "  \"glaive_toolcall_en_demo\": {\n",
    "    \"file_name\": \"glaive_toolcall_en_demo.json\",\n",
    "    \"formatting\": \"sharegpt\",\n",
    "    \"columns\": {\n",
    "      \"messages\": \"conversations\",\n",
    "      \"tools\": \"tools\"\n",
    "    }\n",
    "  },\n",
    "  \"glaive_toolcall_zh_demo\": {\n",
    "    \"file_name\": \"glaive_toolcall_zh_demo.json\",\n",
    "    \"formatting\": \"sharegpt\",\n",
    "    \"columns\": {\n",
    "      \"messages\": \"conversations\",\n",
    "      \"tools\": \"tools\"\n",
    "    }\n",
    "  },\n",
    "  \"mllm_demo\": {\n",
    "    \"file_name\": \"mllm_demo.json\",\n",
    "    \"formatting\": \"sharegpt\",\n",
    "    \"columns\": {\n",
    "      \"messages\": \"messages\",\n",
    "      \"images\": \"images\"\n",
    "    },\n",
    "    \"tags\": {\n",
    "      \"role_tag\": \"role\",\n",
    "      \"content_tag\": \"content\",\n",
    "      \"user_tag\": \"user\",\n",
    "      \"assistant_tag\": \"assistant\"\n",
    "    }\n",
    "  },\n",
    "  \"mllm_audio_demo\": {\n",
    "    \"file_name\": \"mllm_audio_demo.json\",\n",
    "    \"formatting\": \"sharegpt\",\n",
    "    \"columns\": {\n",
    "      \"messages\": \"messages\",\n",
    "      \"audios\": \"audios\"\n",
    "    },\n",
    "    \"tags\": {\n",
    "      \"role_tag\": \"role\",\n",
    "      \"content_tag\": \"content\",\n",
    "      \"user_tag\": \"user\",\n",
    "      \"assistant_tag\": \"assistant\"\n",
    "    }\n",
    "  },\n",
    "  \"mllm_video_demo\": {\n",
    "    \"file_name\": \"mllm_video_demo.json\",\n",
    "    \"formatting\": \"sharegpt\",\n",
    "    \"columns\": {\n",
    "      \"messages\": \"messages\",\n",
    "      \"videos\": \"videos\"\n",
    "    },\n",
    "    \"tags\": {\n",
    "      \"role_tag\": \"role\",\n",
    "      \"content_tag\": \"content\",\n",
    "      \"user_tag\": \"user\",\n",
    "      \"assistant_tag\": \"assistant\"\n",
    "    }\n",
    "  },\n",
    "  \"mllm_video_audio_demo\": {\n",
    "    \"file_name\": \"mllm_video_audio_demo.json\",\n",
    "    \"formatting\": \"sharegpt\",\n",
    "    \"columns\": {\n",
    "      \"messages\": \"messages\",\n",
    "      \"videos\": \"videos\",\n",
    "      \"audios\": \"audios\"\n",
    "    },\n",
    "    \"tags\": {\n",
    "      \"role_tag\": \"role\",\n",
    "      \"content_tag\": \"content\",\n",
    "      \"user_tag\": \"user\",\n",
    "      \"assistant_tag\": \"assistant\"\n",
    "    }\n",
    "  },\n",
    "  \"alpaca_en\": {\n",
    "    \"hf_hub_url\": \"llamafactory/alpaca_en\",\n",
    "    \"ms_hub_url\": \"llamafactory/alpaca_en\",\n",
    "    \"om_hub_url\": \"HaM/alpaca_en\"\n",
    "  },\n",
    "  \"alpaca_zh\": {\n",
    "    \"hf_hub_url\": \"llamafactory/alpaca_zh\",\n",
    "    \"ms_hub_url\": \"llamafactory/alpaca_zh\"\n",
    "  },\n",
    "  \"alpaca_gpt4_en\": {\n",
    "    \"hf_hub_url\": \"llamafactory/alpaca_gpt4_en\",\n",
    "    \"ms_hub_url\": \"llamafactory/alpaca_gpt4_en\"\n",
    "  },\n",
    "  \"alpaca_gpt4_zh\": {\n",
    "    \"hf_hub_url\": \"llamafactory/alpaca_gpt4_zh\",\n",
    "    \"ms_hub_url\": \"llamafactory/alpaca_gpt4_zh\",\n",
    "    \"om_hub_url\": \"State_Cloud/alpaca-gpt4-data-zh\"\n",
    "  },\n",
    "  \"glaive_toolcall_en\": {\n",
    "    \"hf_hub_url\": \"llamafactory/glaive_toolcall_en\",\n",
    "    \"formatting\": \"sharegpt\",\n",
    "    \"columns\": {\n",
    "      \"messages\": \"conversations\",\n",
    "      \"tools\": \"tools\"\n",
    "    }\n",
    "  },\n",
    "  \"glaive_toolcall_zh\": {\n",
    "    \"hf_hub_url\": \"llamafactory/glaive_toolcall_zh\",\n",
    "    \"formatting\": \"sharegpt\",\n",
    "    \"columns\": {\n",
    "      \"messages\": \"conversations\",\n",
    "      \"tools\": \"tools\"\n",
    "    }\n",
    "  },\n",
    "  \"lima\": {\n",
    "    \"hf_hub_url\": \"llamafactory/lima\",\n",
    "    \"formatting\": \"sharegpt\"\n",
    "  },\n",
    "  \"guanaco\": {\n",
    "    \"hf_hub_url\": \"JosephusCheung/GuanacoDataset\",\n",
    "    \"ms_hub_url\": \"AI-ModelScope/GuanacoDataset\"\n",
    "  },\n",
    "  \"belle_2m\": {\n",
    "    \"hf_hub_url\": \"BelleGroup/train_2M_CN\",\n",
    "    \"ms_hub_url\": \"AI-ModelScope/train_2M_CN\"\n",
    "  },\n",
    "  \"belle_1m\": {\n",
    "    \"hf_hub_url\": \"BelleGroup/train_1M_CN\",\n",
    "    \"ms_hub_url\": \"AI-ModelScope/train_1M_CN\"\n",
    "  },\n",
    "  \"belle_0.5m\": {\n",
    "    \"hf_hub_url\": \"BelleGroup/train_0.5M_CN\",\n",
    "    \"ms_hub_url\": \"AI-ModelScope/train_0.5M_CN\"\n",
    "  },\n",
    "  \"belle_dialog\": {\n",
    "    \"hf_hub_url\": \"BelleGroup/generated_chat_0.4M\",\n",
    "    \"ms_hub_url\": \"AI-ModelScope/generated_chat_0.4M\"\n",
    "  },\n",
    "  \"belle_math\": {\n",
    "    \"hf_hub_url\": \"BelleGroup/school_math_0.25M\",\n",
    "    \"ms_hub_url\": \"AI-ModelScope/school_math_0.25M\"\n",
    "  },\n",
    "  \"open_platypus\": {\n",
    "    \"hf_hub_url\": \"garage-bAInd/Open-Platypus\",\n",
    "    \"ms_hub_url\": \"AI-ModelScope/Open-Platypus\"\n",
    "  },\n",
    "  \"codealpaca\": {\n",
    "    \"hf_hub_url\": \"sahil2801/CodeAlpaca-20k\",\n",
    "    \"ms_hub_url\": \"AI-ModelScope/CodeAlpaca-20k\"\n",
    "  },\n",
    "  \"alpaca_cot\": {\n",
    "    \"hf_hub_url\": \"QingyiSi/Alpaca-CoT\",\n",
    "    \"ms_hub_url\": \"AI-ModelScope/Alpaca-CoT\"\n",
    "  },\n",
    "  \"openorca\": {\n",
    "    \"hf_hub_url\": \"Open-Orca/OpenOrca\",\n",
    "    \"ms_hub_url\": \"AI-ModelScope/OpenOrca\",\n",
    "    \"columns\": {\n",
    "      \"prompt\": \"question\",\n",
    "      \"response\": \"response\",\n",
    "      \"system\": \"system_prompt\"\n",
    "    }\n",
    "  },\n",
    "  \"slimorca\": {\n",
    "    \"hf_hub_url\": \"Open-Orca/SlimOrca\",\n",
    "    \"formatting\": \"sharegpt\"\n",
    "  },\n",
    "  \"mathinstruct\": {\n",
    "    \"hf_hub_url\": \"TIGER-Lab/MathInstruct\",\n",
    "    \"ms_hub_url\": \"AI-ModelScope/MathInstruct\",\n",
    "    \"columns\": {\n",
    "      \"prompt\": \"instruction\",\n",
    "      \"response\": \"output\"\n",
    "    }\n",
    "  },\n",
    "  \"firefly\": {\n",
    "    \"hf_hub_url\": \"YeungNLP/firefly-train-1.1M\",\n",
    "    \"columns\": {\n",
    "      \"prompt\": \"input\",\n",
    "      \"response\": \"target\"\n",
    "    }\n",
    "  },\n",
    "  \"wikiqa\": {\n",
    "    \"hf_hub_url\": \"wiki_qa\",\n",
    "    \"columns\": {\n",
    "      \"prompt\": \"question\",\n",
    "      \"response\": \"answer\"\n",
    "    }\n",
    "  },\n",
    "  \"webqa\": {\n",
    "    \"hf_hub_url\": \"suolyer/webqa\",\n",
    "    \"ms_hub_url\": \"AI-ModelScope/webqa\",\n",
    "    \"columns\": {\n",
    "      \"prompt\": \"input\",\n",
    "      \"response\": \"output\"\n",
    "    }\n",
    "  },\n",
    "  \"webnovel\": {\n",
    "    \"hf_hub_url\": \"zxbsmk/webnovel_cn\",\n",
    "    \"ms_hub_url\": \"AI-ModelScope/webnovel_cn\"\n",
    "  },\n",
    "  \"nectar_sft\": {\n",
    "    \"hf_hub_url\": \"AstraMindAI/SFT-Nectar\",\n",
    "    \"ms_hub_url\": \"AI-ModelScope/SFT-Nectar\"\n",
    "  },\n",
    "  \"deepctrl\": {\n",
    "    \"ms_hub_url\": \"deepctrl/deepctrl-sft-data\"\n",
    "  },\n",
    "  \"adgen_train\": {\n",
    "    \"hf_hub_url\": \"HasturOfficial/adgen\",\n",
    "    \"ms_hub_url\": \"AI-ModelScope/adgen\",\n",
    "    \"split\": \"train\",\n",
    "    \"columns\": {\n",
    "      \"prompt\": \"content\",\n",
    "      \"response\": \"summary\"\n",
    "    }\n",
    "  },\n",
    "  \"adgen_eval\": {\n",
    "    \"hf_hub_url\": \"HasturOfficial/adgen\",\n",
    "    \"ms_hub_url\": \"AI-ModelScope/adgen\",\n",
    "    \"split\": \"validation\",\n",
    "    \"columns\": {\n",
    "      \"prompt\": \"content\",\n",
    "      \"response\": \"summary\"\n",
    "    }\n",
    "  },\n",
    "  \"sharegpt_hyper\": {\n",
    "    \"hf_hub_url\": \"totally-not-an-llm/sharegpt-hyperfiltered-3k\",\n",
    "    \"formatting\": \"sharegpt\"\n",
    "  },\n",
    "  \"sharegpt4\": {\n",
    "    \"hf_hub_url\": \"shibing624/sharegpt_gpt4\",\n",
    "    \"ms_hub_url\": \"AI-ModelScope/sharegpt_gpt4\",\n",
    "    \"formatting\": \"sharegpt\"\n",
    "  },\n",
    "  \"ultrachat_200k\": {\n",
    "    \"hf_hub_url\": \"HuggingFaceH4/ultrachat_200k\",\n",
    "    \"ms_hub_url\": \"AI-ModelScope/ultrachat_200k\",\n",
    "    \"split\": \"train_sft\",\n",
    "    \"formatting\": \"sharegpt\",\n",
    "    \"columns\": {\n",
    "      \"messages\": \"messages\"\n",
    "    },\n",
    "    \"tags\": {\n",
    "      \"role_tag\": \"role\",\n",
    "      \"content_tag\": \"content\",\n",
    "      \"user_tag\": \"user\",\n",
    "      \"assistant_tag\": \"assistant\"\n",
    "    }\n",
    "  },\n",
    "  \"infinity_instruct\": {\n",
    "    \"hf_hub_url\": \"BAAI/Infinity-Instruct\",\n",
    "    \"formatting\": \"sharegpt\"\n",
    "  },\n",
    "  \"agent_instruct\": {\n",
    "    \"hf_hub_url\": \"THUDM/AgentInstruct\",\n",
    "    \"ms_hub_url\": \"ZhipuAI/AgentInstruct\",\n",
    "    \"formatting\": \"sharegpt\"\n",
    "  },\n",
    "  \"lmsys_chat\": {\n",
    "    \"hf_hub_url\": \"lmsys/lmsys-chat-1m\",\n",
    "    \"ms_hub_url\": \"AI-ModelScope/lmsys-chat-1m\",\n",
    "    \"formatting\": \"sharegpt\",\n",
    "    \"columns\": {\n",
    "      \"messages\": \"conversation\"\n",
    "    },\n",
    "    \"tags\": {\n",
    "      \"role_tag\": \"role\",\n",
    "      \"content_tag\": \"content\",\n",
    "      \"user_tag\": \"user\",\n",
    "      \"assistant_tag\": \"assistant\"\n",
    "    }\n",
    "  },\n",
    "  \"evol_instruct\": {\n",
    "    \"hf_hub_url\": \"WizardLM/WizardLM_evol_instruct_V2_196k\",\n",
    "    \"ms_hub_url\": \"AI-ModelScope/WizardLM_evol_instruct_V2_196k\",\n",
    "    \"formatting\": \"sharegpt\"\n",
    "  },\n",
    "  \"glaive_toolcall_100k\": {\n",
    "    \"hf_hub_url\": \"hiyouga/glaive-function-calling-v2-sharegpt\",\n",
    "    \"formatting\": \"sharegpt\",\n",
    "    \"columns\": {\n",
    "      \"messages\": \"conversations\",\n",
    "      \"tools\": \"tools\"\n",
    "    }\n",
    "  },\n",
    "  \"cosmopedia\": {\n",
    "    \"hf_hub_url\": \"HuggingFaceTB/cosmopedia\",\n",
    "    \"columns\": {\n",
    "      \"prompt\": \"prompt\",\n",
    "      \"response\": \"text\"\n",
    "    }\n",
    "  },\n",
    "  \"stem_zh\": {\n",
    "    \"hf_hub_url\": \"hfl/stem_zh_instruction\"\n",
    "  },\n",
    "  \"ruozhiba_gpt4\": {\n",
    "    \"hf_hub_url\": \"hfl/ruozhiba_gpt4_turbo\"\n",
    "  },\n",
    "  \"neo_sft\": {\n",
    "    \"hf_hub_url\": \"m-a-p/neo_sft_phase2\",\n",
    "    \"formatting\": \"sharegpt\"\n",
    "  },\n",
    "  \"magpie_pro_300k\": {\n",
    "    \"hf_hub_url\": \"Magpie-Align/Magpie-Pro-300K-Filtered\",\n",
    "    \"formatting\": \"sharegpt\"\n",
    "  },\n",
    "  \"magpie_ultra\": {\n",
    "    \"hf_hub_url\": \"argilla/magpie-ultra-v0.1\",\n",
    "    \"columns\": {\n",
    "      \"prompt\": \"instruction\",\n",
    "      \"response\": \"response\"\n",
    "    }\n",
    "  },\n",
    "  \"web_instruct\": {\n",
    "    \"hf_hub_url\": \"TIGER-Lab/WebInstructSub\",\n",
    "    \"columns\": {\n",
    "      \"prompt\": \"question\",\n",
    "      \"response\": \"answer\"\n",
    "    }\n",
    "  },\n",
    "  \"openo1_sft\": {\n",
    "    \"hf_hub_url\": \"llamafactory/OpenO1-SFT\",\n",
    "    \"ms_hub_url\": \"llamafactory/OpenO1-SFT\",\n",
    "    \"columns\": {\n",
    "      \"prompt\": \"prompt\",\n",
    "      \"response\": \"response\"\n",
    "    }\n",
    "  },\n",
    "  \"open_thoughts\": {\n",
    "    \"hf_hub_url\": \"llamafactory/OpenThoughts-114k\",\n",
    "    \"formatting\": \"sharegpt\",\n",
    "    \"columns\": {\n",
    "      \"messages\": \"messages\"\n",
    "    },\n",
    "    \"tags\": {\n",
    "      \"role_tag\": \"role\",\n",
    "      \"content_tag\": \"content\",\n",
    "      \"user_tag\": \"user\",\n",
    "      \"assistant_tag\": \"assistant\",\n",
    "      \"system_tag\": \"system\"\n",
    "    }\n",
    "  },\n",
    "  \"open_r1_math\": {\n",
    "    \"hf_hub_url\": \"llamafactory/OpenR1-Math-94k\",\n",
    "    \"formatting\": \"sharegpt\",\n",
    "    \"columns\": {\n",
    "      \"messages\": \"messages\"\n",
    "    },\n",
    "    \"tags\": {\n",
    "      \"role_tag\": \"role\",\n",
    "      \"content_tag\": \"content\",\n",
    "      \"user_tag\": \"user\",\n",
    "      \"assistant_tag\": \"assistant\",\n",
    "      \"system_tag\": \"system\"\n",
    "    }\n",
    "  },\n",
    "  \"chinese_r1_distill\": {\n",
    "    \"hf_hub_url\": \"Congliu/Chinese-DeepSeek-R1-Distill-data-110k-SFT\",\n",
    "    \"ms_hub_url\": \"liucong/Chinese-DeepSeek-R1-Distill-data-110k-SFT\"\n",
    "  },\n",
    "  \"llava_1k_en\": {\n",
    "    \"hf_hub_url\": \"BUAADreamer/llava-en-zh-2k\",\n",
    "    \"subset\": \"en\",\n",
    "    \"formatting\": \"sharegpt\",\n",
    "    \"columns\": {\n",
    "      \"messages\": \"messages\",\n",
    "      \"images\": \"images\"\n",
    "    },\n",
    "    \"tags\": {\n",
    "      \"role_tag\": \"role\",\n",
    "      \"content_tag\": \"content\",\n",
    "      \"user_tag\": \"user\",\n",
    "      \"assistant_tag\": \"assistant\"\n",
    "    }\n",
    "  },\n",
    "  \"llava_1k_zh\": {\n",
    "    \"hf_hub_url\": \"BUAADreamer/llava-en-zh-2k\",\n",
    "    \"subset\": \"zh\",\n",
    "    \"formatting\": \"sharegpt\",\n",
    "    \"columns\": {\n",
    "      \"messages\": \"messages\",\n",
    "      \"images\": \"images\"\n",
    "    },\n",
    "    \"tags\": {\n",
    "      \"role_tag\": \"role\",\n",
    "      \"content_tag\": \"content\",\n",
    "      \"user_tag\": \"user\",\n",
    "      \"assistant_tag\": \"assistant\"\n",
    "    }\n",
    "  },\n",
    "  \"llava_150k_en\": {\n",
    "    \"hf_hub_url\": \"BUAADreamer/llava-en-zh-300k\",\n",
    "    \"subset\": \"en\",\n",
    "    \"formatting\": \"sharegpt\",\n",
    "    \"columns\": {\n",
    "      \"messages\": \"messages\",\n",
    "      \"images\": \"images\"\n",
    "    },\n",
    "    \"tags\": {\n",
    "      \"role_tag\": \"role\",\n",
    "      \"content_tag\": \"content\",\n",
    "      \"user_tag\": \"user\",\n",
    "      \"assistant_tag\": \"assistant\"\n",
    "    }\n",
    "  },\n",
    "  \"llava_150k_zh\": {\n",
    "    \"hf_hub_url\": \"BUAADreamer/llava-en-zh-300k\",\n",
    "    \"subset\": \"zh\",\n",
    "    \"formatting\": \"sharegpt\",\n",
    "    \"columns\": {\n",
    "      \"messages\": \"messages\",\n",
    "      \"images\": \"images\"\n",
    "    },\n",
    "    \"tags\": {\n",
    "      \"role_tag\": \"role\",\n",
    "      \"content_tag\": \"content\",\n",
    "      \"user_tag\": \"user\",\n",
    "      \"assistant_tag\": \"assistant\"\n",
    "    }\n",
    "  },\n",
    "  \"pokemon_cap\": {\n",
    "    \"hf_hub_url\": \"llamafactory/pokemon-gpt4o-captions\",\n",
    "    \"formatting\": \"sharegpt\",\n",
    "    \"columns\": {\n",
    "      \"messages\": \"conversations\",\n",
    "      \"images\": \"images\"\n",
    "    }\n",
    "  },\n",
    "  \"mllm_pt_demo\": {\n",
    "    \"hf_hub_url\": \"BUAADreamer/mllm_pt_demo\",\n",
    "    \"formatting\": \"sharegpt\",\n",
    "    \"columns\": {\n",
    "      \"messages\": \"messages\",\n",
    "      \"images\": \"images\"\n",
    "    },\n",
    "    \"tags\": {\n",
    "      \"role_tag\": \"role\",\n",
    "      \"content_tag\": \"content\",\n",
    "      \"user_tag\": \"user\",\n",
    "      \"assistant_tag\": \"assistant\"\n",
    "    }\n",
    "  },\n",
    "  \"oasst_de\": {\n",
    "    \"hf_hub_url\": \"mayflowergmbh/oasst_de\"\n",
    "  },\n",
    "  \"dolly_15k_de\": {\n",
    "    \"hf_hub_url\": \"mayflowergmbh/dolly-15k_de\"\n",
    "  },\n",
    "  \"alpaca-gpt4_de\": {\n",
    "    \"hf_hub_url\": \"mayflowergmbh/alpaca-gpt4_de\"\n",
    "  },\n",
    "  \"openschnabeltier_de\": {\n",
    "    \"hf_hub_url\": \"mayflowergmbh/openschnabeltier_de\"\n",
    "  },\n",
    "  \"evol_instruct_de\": {\n",
    "    \"hf_hub_url\": \"mayflowergmbh/evol-instruct_de\"\n",
    "  },\n",
    "  \"dolphin_de\": {\n",
    "    \"hf_hub_url\": \"mayflowergmbh/dolphin_de\"\n",
    "  },\n",
    "  \"booksum_de\": {\n",
    "    \"hf_hub_url\": \"mayflowergmbh/booksum_de\"\n",
    "  },\n",
    "  \"airoboros_de\": {\n",
    "    \"hf_hub_url\": \"mayflowergmbh/airoboros-3.0_de\"\n",
    "  },\n",
    "  \"ultrachat_de\": {\n",
    "    \"hf_hub_url\": \"mayflowergmbh/ultra-chat_de\"\n",
    "  },\n",
    "  \"dlr_web\": {\n",
    "    \"hf_hub_url\": \"Attention1115/DLR-Web\",\n",
    "    \"split\": \"full\",\n",
    "    \"columns\": {\n",
    "      \"prompt\": \"question\",\n",
    "      \"response\": \"response\"\n",
    "    }\n",
    "  },\n",
    "  \"dpo_en_demo\": {\n",
    "    \"file_name\": \"dpo_en_demo.json\",\n",
    "    \"ranking\": True,\n",
    "    \"formatting\": \"sharegpt\",\n",
    "    \"columns\": {\n",
    "      \"messages\": \"conversations\",\n",
    "      \"chosen\": \"chosen\",\n",
    "      \"rejected\": \"rejected\"\n",
    "    }\n",
    "  },\n",
    "  \"dpo_zh_demo\": {\n",
    "    \"file_name\": \"dpo_zh_demo.json\",\n",
    "    \"ranking\": True,\n",
    "    \"formatting\": \"sharegpt\",\n",
    "    \"columns\": {\n",
    "      \"messages\": \"conversations\",\n",
    "      \"chosen\": \"chosen\",\n",
    "      \"rejected\": \"rejected\"\n",
    "    }\n",
    "  },\n",
    "  \"dpo_mix_en\": {\n",
    "    \"hf_hub_url\": \"llamafactory/DPO-En-Zh-20k\",\n",
    "    \"subset\": \"en\",\n",
    "    \"ranking\": True,\n",
    "    \"formatting\": \"sharegpt\",\n",
    "    \"columns\": {\n",
    "      \"messages\": \"conversations\",\n",
    "      \"chosen\": \"chosen\",\n",
    "      \"rejected\": \"rejected\"\n",
    "    }\n",
    "  },\n",
    "  \"dpo_mix_zh\": {\n",
    "    \"hf_hub_url\": \"llamafactory/DPO-En-Zh-20k\",\n",
    "    \"subset\": \"zh\",\n",
    "    \"ranking\": True,\n",
    "    \"formatting\": \"sharegpt\",\n",
    "    \"columns\": {\n",
    "      \"messages\": \"conversations\",\n",
    "      \"chosen\": \"chosen\",\n",
    "      \"rejected\": \"rejected\"\n",
    "    }\n",
    "  },\n",
    "  \"ultrafeedback\": {\n",
    "    \"hf_hub_url\": \"llamafactory/ultrafeedback_binarized\",\n",
    "    \"ms_hub_url\": \"llamafactory/ultrafeedback_binarized\",\n",
    "    \"ranking\": True,\n",
    "    \"columns\": {\n",
    "      \"prompt\": \"instruction\",\n",
    "      \"chosen\": \"chosen\",\n",
    "      \"rejected\": \"rejected\"\n",
    "    }\n",
    "  },\n",
    "  \"coig_p\": {\n",
    "    \"hf_hub_url\": \"m-a-p/COIG-P\",\n",
    "    \"ranking\": True,\n",
    "    \"formatting\": \"sharegpt\",\n",
    "    \"columns\": {\n",
    "      \"messages\": \"conversations\",\n",
    "      \"chosen\": \"chosen\",\n",
    "      \"rejected\": \"rejected\"\n",
    "    }\n",
    "  },\n",
    "  \"rlhf_v\": {\n",
    "    \"hf_hub_url\": \"llamafactory/RLHF-V\",\n",
    "    \"ranking\": True,\n",
    "    \"formatting\": \"sharegpt\",\n",
    "    \"columns\": {\n",
    "      \"messages\": \"conversations\",\n",
    "      \"chosen\": \"chosen\",\n",
    "      \"rejected\": \"rejected\",\n",
    "      \"images\": \"images\"\n",
    "    }\n",
    "  },\n",
    "  \"vlfeedback\": {\n",
    "    \"hf_hub_url\": \"Zhihui/VLFeedback\",\n",
    "    \"ranking\": True,\n",
    "    \"formatting\": \"sharegpt\",\n",
    "    \"columns\": {\n",
    "      \"messages\": \"conversations\",\n",
    "      \"chosen\": \"chosen\",\n",
    "      \"rejected\": \"rejected\",\n",
    "      \"images\": \"images\"\n",
    "    }\n",
    "  },\n",
    "  \"rlaif_v\": {\n",
    "    \"hf_hub_url\": \"openbmb/RLAIF-V-Dataset\",\n",
    "    \"ranking\": True,\n",
    "    \"columns\": {\n",
    "      \"prompt\": \"question\",\n",
    "      \"chosen\": \"chosen\",\n",
    "      \"rejected\": \"rejected\",\n",
    "      \"images\": \"image\"\n",
    "    }\n",
    "  },\n",
    "  \"orca_pairs\": {\n",
    "    \"hf_hub_url\": \"Intel/orca_dpo_pairs\",\n",
    "    \"ranking\": True,\n",
    "    \"columns\": {\n",
    "      \"prompt\": \"question\",\n",
    "      \"chosen\": \"chosen\",\n",
    "      \"rejected\": \"rejected\",\n",
    "      \"system\": \"system\"\n",
    "    }\n",
    "  },\n",
    "  \"nectar_rm\": {\n",
    "    \"hf_hub_url\": \"AstraMindAI/RLAIF-Nectar\",\n",
    "    \"ms_hub_url\": \"AI-ModelScope/RLAIF-Nectar\",\n",
    "    \"ranking\": True\n",
    "  },\n",
    "  \"orca_dpo_de\": {\n",
    "    \"hf_hub_url\": \"mayflowergmbh/intel_orca_dpo_pairs_de\",\n",
    "    \"ranking\": True\n",
    "  },\n",
    "  \"kto_en_demo\": {\n",
    "    \"file_name\": \"kto_en_demo.json\",\n",
    "    \"formatting\": \"sharegpt\",\n",
    "    \"columns\": {\n",
    "      \"messages\": \"messages\",\n",
    "      \"kto_tag\": \"label\"\n",
    "    },\n",
    "    \"tags\": {\n",
    "      \"role_tag\": \"role\",\n",
    "      \"content_tag\": \"content\",\n",
    "      \"user_tag\": \"user\",\n",
    "      \"assistant_tag\": \"assistant\"\n",
    "    }\n",
    "  },\n",
    "  \"kto_mix_en\": {\n",
    "    \"hf_hub_url\": \"argilla/kto-mix-15k\",\n",
    "    \"formatting\": \"sharegpt\",\n",
    "    \"columns\": {\n",
    "      \"messages\": \"completion\",\n",
    "      \"kto_tag\": \"label\"\n",
    "    },\n",
    "    \"tags\": {\n",
    "      \"role_tag\": \"role\",\n",
    "      \"content_tag\": \"content\",\n",
    "      \"user_tag\": \"user\",\n",
    "      \"assistant_tag\": \"assistant\"\n",
    "    }\n",
    "  },\n",
    "  \"ultrafeedback_kto\": {\n",
    "    \"hf_hub_url\": \"argilla/ultrafeedback-binarized-preferences-cleaned-kto\",\n",
    "    \"ms_hub_url\": \"AI-ModelScope/ultrafeedback-binarized-preferences-cleaned-kto\",\n",
    "    \"columns\": {\n",
    "      \"prompt\": \"prompt\",\n",
    "      \"response\": \"completion\",\n",
    "      \"kto_tag\": \"label\"\n",
    "    }\n",
    "  },\n",
    "  \"wiki_demo\": {\n",
    "    \"file_name\": \"wiki_demo.txt\",\n",
    "    \"columns\": {\n",
    "      \"prompt\": \"text\"\n",
    "    }\n",
    "  },\n",
    "  \"c4_demo\": {\n",
    "    \"file_name\": \"c4_demo.jsonl\",\n",
    "    \"columns\": {\n",
    "      \"prompt\": \"text\"\n",
    "    }\n",
    "  },\n",
    "  \"refinedweb\": {\n",
    "    \"hf_hub_url\": \"tiiuae/falcon-refinedweb\",\n",
    "    \"columns\": {\n",
    "      \"prompt\": \"content\"\n",
    "    }\n",
    "  },\n",
    "  \"redpajama_v2\": {\n",
    "    \"hf_hub_url\": \"togethercomputer/RedPajama-Data-V2\",\n",
    "    \"columns\": {\n",
    "      \"prompt\": \"raw_content\"\n",
    "    },\n",
    "    \"subset\": \"default\"\n",
    "  },\n",
    "  \"wikipedia_en\": {\n",
    "    \"hf_hub_url\": \"olm/olm-wikipedia-20221220\",\n",
    "    \"ms_hub_url\": \"AI-ModelScope/olm-wikipedia-20221220\",\n",
    "    \"columns\": {\n",
    "      \"prompt\": \"text\"\n",
    "    }\n",
    "  },\n",
    "  \"wikipedia_zh\": {\n",
    "    \"hf_hub_url\": \"pleisto/wikipedia-cn-20230720-filtered\",\n",
    "    \"ms_hub_url\": \"AI-ModelScope/wikipedia-cn-20230720-filtered\",\n",
    "    \"columns\": {\n",
    "      \"prompt\": \"completion\"\n",
    "    }\n",
    "  },\n",
    "  \"pile\": {\n",
    "    \"hf_hub_url\": \"monology/pile-uncopyrighted\",\n",
    "    \"ms_hub_url\": \"AI-ModelScope/pile\",\n",
    "    \"columns\": {\n",
    "      \"prompt\": \"text\"\n",
    "    }\n",
    "  },\n",
    "  \"skypile\": {\n",
    "    \"hf_hub_url\": \"Skywork/SkyPile-150B\",\n",
    "    \"ms_hub_url\": \"AI-ModelScope/SkyPile-150B\",\n",
    "    \"columns\": {\n",
    "      \"prompt\": \"text\"\n",
    "    }\n",
    "  },\n",
    "  \"fineweb\": {\n",
    "    \"hf_hub_url\": \"HuggingFaceFW/fineweb\",\n",
    "    \"columns\": {\n",
    "      \"prompt\": \"text\"\n",
    "    }\n",
    "  },\n",
    "  \"fineweb_edu\": {\n",
    "    \"hf_hub_url\": \"HuggingFaceFW/fineweb-edu\",\n",
    "    \"columns\": {\n",
    "      \"prompt\": \"text\"\n",
    "    }\n",
    "  },\n",
    "  \"cci3_hq\": {\n",
    "    \"hf_hub_url\": \"BAAI/CCI3-HQ\",\n",
    "    \"columns\": {\n",
    "      \"prompt\": \"text\"\n",
    "    }\n",
    "  },\n",
    "  \"cci3_data\": {\n",
    "    \"hf_hub_url\": \"BAAI/CCI3-Data\",\n",
    "    \"columns\": {\n",
    "      \"prompt\": \"text\"\n",
    "    }\n",
    "  },\n",
    "  \"cci4_base\": {\n",
    "    \"hf_hub_url\": \"BAAI/CCI4.0-M2-Base-v1\",\n",
    "    \"columns\": {\n",
    "      \"prompt\": \"text\"\n",
    "    }\n",
    "  },\n",
    "  \"cci4_cot\": {\n",
    "    \"hf_hub_url\": \"BAAI/CCI4.0-M2-CoT-v1\",\n",
    "    \"columns\": {\n",
    "      \"prompt\": \"text\"\n",
    "    }\n",
    "  },\n",
    "  \"cci4_extra\": {\n",
    "    \"hf_hub_url\": \"BAAI/CCI4.0-M2-Extra-v1\",\n",
    "    \"columns\": {\n",
    "      \"prompt\": \"text\"\n",
    "    }\n",
    "  },\n",
    "  \"the_stack\": {\n",
    "    \"hf_hub_url\": \"bigcode/the-stack\",\n",
    "    \"ms_hub_url\": \"AI-ModelScope/the-stack\",\n",
    "    \"columns\": {\n",
    "      \"prompt\": \"content\"\n",
    "    }\n",
    "  },\n",
    "  \"starcoder_python\": {\n",
    "    \"hf_hub_url\": \"bigcode/starcoderdata\",\n",
    "    \"ms_hub_url\": \"AI-ModelScope/starcoderdata\",\n",
    "    \"columns\": {\n",
    "      \"prompt\": \"content\"\n",
    "    },\n",
    "    \"folder\": \"python\"\n",
    "  },\n",
    "  \"dpo_finetuning_train\": {\n",
    "    \"file_name\": \"/kaggle/working/train-v1-edited.json\",\n",
    "    \"formatting\": \"sharegpt\",\n",
    "    \"ranking\": True,\n",
    "    \"columns\": {\n",
    "      \"messages\": \"conversations\",\n",
    "      \"chosen\": \"chosen\",\n",
    "      \"rejected\": \"rejected\",\n",
    "      \"images\": \"images\"\n",
    "   }\n",
    "  },\n",
    "    \"dpo_finetuning_test\": {\n",
    "    \"file_name\": \"/kaggle/working/val-v1-edited.json\",\n",
    "    \"formatting\": \"sharegpt\",\n",
    "    \"ranking\": True,\n",
    "    \"columns\": {\n",
    "      \"messages\": \"conversations\",\n",
    "      \"chosen\": \"chosen\",\n",
    "      \"rejected\": \"rejected\",\n",
    "      \"images\": \"images\"\n",
    "   }\n",
    "  }\n",
    "}\n",
    "\n",
    "with open(\"/kaggle/working/LlamaFactory/data/dataset_info.json\",\"w\") as dest:\n",
    "    json.dump(data_info, dest, ensure_ascii=False, default=str, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Write the Training Configuration YAML\n",
    "\n",
    "### What is a YAML config?\n",
    "Instead of long Python scripts with dozens of arguments, LlamaFactory uses a **single YAML file** that defines everything about the training run. This makes experiments easy to track, share, and reproduce.\n",
    "\n",
    "### üèóÔ∏è Model Section\n",
    "\n",
    "| Parameter | Value | Explanation |\n",
    "|---|---|---|\n",
    "| `model_name_or_path` | `Qwen/Qwen3-VL-4B-Instruct` | HuggingFace model ID ‚Äî auto-downloaded |\n",
    "| `image_max_pixels` | `262144` | Max image resolution = 512√ó512 pixels (512¬≤=262144). Larger = more detail but more VRAM |\n",
    "| `video_max_pixels` | `16384` | Max video frame resolution ‚Äî set low since we don't use video |\n",
    "| `trust_remote_code` | `true` | Allow executing custom model code from HuggingFace (required for Qwen3-VL) |\n",
    "\n",
    "### üéØ Method Section\n",
    "\n",
    "| Parameter | Value | Explanation |\n",
    "|---|---|---|\n",
    "| `stage` | `dpo` | Training stage: Direct Preference Optimization |\n",
    "| `do_train` | `true` | Enable training (vs. inference only) |\n",
    "| `finetuning_type` | `lora` | Use LoRA adapters instead of full fine-tuning |\n",
    "| `lora_rank` | `32` | LoRA rank ‚Äî higher rank = more parameters = more expressive but uses more VRAM |\n",
    "| `lora_target` | `all` | Apply LoRA to ALL linear layers in the model |\n",
    "| `pref_beta` | `0.1` | DPO temperature ‚Äî controls how strongly the model is pushed toward chosen responses |\n",
    "| `pref_loss` | `sigmoid` | DPO loss type: standard sigmoid DPO (as in the original DPO paper) |\n",
    "\n",
    "### üìö Dataset Section\n",
    "\n",
    "| Parameter | Value | Explanation |\n",
    "|---|---|---|\n",
    "| `dataset` | `dpo_finetuning_train` | Name matching our registry entry for training |\n",
    "| `eval_dataset` | `dpo_finetuning_test` | Name matching our registry entry for evaluation |\n",
    "| `template` | `qwen3_vl_nothink` | Chat template ‚Äî **no-thinking mode** means direct answers without `<think>` tokens |\n",
    "| `cutoff_len` | `12000` | Maximum sequence length in tokens ‚Äî longer sequences are truncated |\n",
    "| `preprocessing_num_workers` | `16` | CPU workers for data preprocessing |\n",
    "| `dataloader_num_workers` | `4` | CPU workers for feeding data to GPU during training |\n",
    "\n",
    "### üîÑ Training Hyperparameters\n",
    "\n",
    "| Parameter | Value | Explanation |\n",
    "|---|---|---|\n",
    "| `per_device_train_batch_size` | `2` | 2 samples per GPU per step |\n",
    "| `gradient_accumulation_steps` | `8` | Accumulate gradients over 8 mini-batches. Effective batch size = 2√ó8 = **16** |\n",
    "| `learning_rate` | `5.0e-6` | Very small LR ‚Äî fine-tuning needs gentle updates to avoid catastrophic forgetting |\n",
    "| `num_train_epochs` | `3.0` | Train for 3 full passes through the dataset |\n",
    "| `lr_scheduler_type` | `cosine` | Cosine annealing ‚Äî LR gradually decays following a cosine curve |\n",
    "| `warmup_ratio` | `0.1` | First 10% of steps: LR linearly warms up from 0 to avoid unstable early updates |\n",
    "| `bf16` | `true` | BFloat16 precision ‚Äî faster than FP32, more stable than FP16 for training |\n",
    "\n",
    "### üìä Output & Evaluation\n",
    "\n",
    "| Parameter | Value | Explanation |\n",
    "|---|---|---|\n",
    "| `output_dir` | `saves/qwen3-vl-4b/lora/dpo` | Where LoRA adapter weights and training artifacts are saved |\n",
    "| `logging_steps` | `25` | Log training loss every 25 steps |\n",
    "| `save_strategy` | `epoch` | Save a checkpoint after each epoch |\n",
    "| `eval_strategy` | `epoch` | Run evaluation after each epoch |\n",
    "| `plot_loss` | `true` | Generate a loss curve plot after training |\n",
    "| `report_to` | `none` | Don't send metrics to W&B, TensorBoard, etc. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "yaml_content = \"\"\"\\\n",
    "### model\n",
    "model_name_or_path: Qwen/Qwen3-VL-4B-Instruct\n",
    "image_max_pixels: 262144\n",
    "video_max_pixels: 16384\n",
    "trust_remote_code: true\n",
    "\n",
    "### method\n",
    "stage: dpo\n",
    "do_train: true\n",
    "finetuning_type: lora\n",
    "lora_rank: 32\n",
    "lora_target: all\n",
    "pref_beta: 0.1\n",
    "pref_loss: sigmoid  # choices: [sigmoid (dpo), orpo, simpo]\n",
    "\n",
    "### dataset\n",
    "dataset: dpo_finetuning_train\n",
    "eval_dataset: dpo_finetuning_test\n",
    "template: qwen3_vl_nothink\n",
    "cutoff_len: 4092\n",
    "max_samples: 1000\n",
    "preprocessing_num_workers: 16\n",
    "dataloader_num_workers: 4\n",
    "\n",
    "### output\n",
    "output_dir: saves/qwen3-vl-4b/lora/dpo\n",
    "logging_strategy: steps         \n",
    "logging_steps: 25\n",
    "save_strategy: epoch\n",
    "plot_loss: true\n",
    "overwrite_output_dir: true\n",
    "report_to: none\n",
    "\n",
    "### train\n",
    "per_device_train_batch_size: 2\n",
    "gradient_accumulation_steps: 8\n",
    "learning_rate: 5.0e-6\n",
    "num_train_epochs: 3.0\n",
    "lr_scheduler_type: cosine\n",
    "warmup_ratio: 0.1\n",
    "bf16: true\n",
    "ddp_timeout: 180000000\n",
    "#resume_from_checkpoint: null\n",
    "\n",
    "### eval\n",
    "per_device_eval_batch_size: 16\n",
    "eval_strategy: epoch\n",
    "\"\"\"\n",
    "\n",
    "with open(\"/kaggle/working/LlamaFactory/examples/train_lora/dpo_finetuning.yaml\", \"w\") as f:\n",
    "    f.write(yaml_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Launch DPO Fine-Tuning\n",
    "\n",
    "### What happens when you run this cell?\n",
    "This is the main training command. LlamaFactory reads the YAML config and orchestrates the entire training process:\n",
    "\n",
    "1. **Downloads the model** ‚Äî Qwen3-VL-4B-Instruct from HuggingFace (~8GB)\n",
    "2. **Loads the dataset** ‚Äî reads `train-v1-edited.json` and `val-v1-edited.json`\n",
    "3. **Preprocesses data** ‚Äî tokenizes text, loads and resizes images, creates input tensors\n",
    "4. **Initializes LoRA** ‚Äî adds trainable adapter layers to the frozen base model\n",
    "5. **Trains for 3 epochs** ‚Äî running DPO loss on (question, chosen, rejected, image) quadruples\n",
    "6. **Evaluates each epoch** ‚Äî computes validation loss to track overfitting\n",
    "7. **Saves checkpoints** ‚Äî saves LoRA adapter weights after each epoch\n",
    "8. **Plots loss curve** ‚Äî generates a PNG showing training and validation loss over time\n",
    "\n",
    "### What is `DISABLE_VERSION_CHECK=1`?\n",
    "Suppresses LlamaFactory's startup version compatibility warnings, which can be noisy when using pinned dependency versions.\n",
    "\n",
    "### What is LoRA?\n",
    "**Low-Rank Adaptation (LoRA)** freezes the original model weights and adds small trainable matrices (adapters) alongside them. This means:\n",
    "- üîµ Only ~1‚Äì5% of total parameters are trained\n",
    "- üíæ Much less VRAM required than full fine-tuning\n",
    "- ‚ö° Faster training\n",
    "- üõ°Ô∏è Less risk of catastrophic forgetting\n",
    "\n",
    "### Expected console output:\n",
    "```\n",
    "[INFO] Loading model: Qwen/Qwen3-VL-4B-Instruct\n",
    "[INFO] trainable params: 83,886,080 || all params: 4,184,166,400 || trainable%: 2.004\n",
    "{'loss': 0.6823, 'learning_rate': 2.5e-06, 'epoch': 1.0}\n",
    "{'eval_loss': 0.6541, 'epoch': 1.0}\n",
    "...\n",
    "[INFO] Training completed. Output saved to saves/qwen3-vl-4b/lora/dpo\n",
    "```\n",
    "\n",
    "### Output files saved:\n",
    "```\n",
    "saves/qwen3-vl-4b/lora/dpo/\n",
    "‚îú‚îÄ‚îÄ adapter_config.json        ‚Üê LoRA architecture config\n",
    "‚îú‚îÄ‚îÄ adapter_model.safetensors  ‚Üê Trained LoRA weights  ‚úÖ most important\n",
    "‚îú‚îÄ‚îÄ training_args.bin          ‚Üê Saved training arguments\n",
    "‚îú‚îÄ‚îÄ trainer_log.jsonl          ‚Üê Step-by-step training metrics\n",
    "‚îî‚îÄ‚îÄ training_loss.png          ‚Üê Loss curve visualization\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!cd LlamaFactory && export DISABLE_VERSION_CHECK=1 && llamafactory-cli train /kaggle/working/LlamaFactory/examples/train_lora/dpo_finetuning.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ ***Inference***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "MODEL_ID = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import Qwen3VLForConditionalGeneration, AutoProcessor\n",
    "\n",
    "# default: Load the model on the available device(s)\n",
    "model = Qwen3VLForConditionalGeneration.from_pretrained(\n",
    "    MODEL_ID, dtype=\"auto\", device_map=\"auto\"\n",
    ")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": \"put-image-path\",\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Preparation for inference\n",
    "inputs = processor.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_dict=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "inputs = inputs.to(model.device)\n",
    "\n",
    "# Inference: Generation of the output\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "print(output_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Summary\n",
    "\n",
    "You have successfully completed a full **DPO fine-tuning pipeline** for a Vision-Language Model!\n",
    "\n",
    "| Step | Task | Output |\n",
    "|---|---|---|\n",
    "| 1 | Load HuggingFace dataset | Raw image+QA DataFrame |\n",
    "| 2 | Process & save images | JPEG files in `imagesdpo/` |\n",
    "| 3 | Split data | 80% train / 20% val |\n",
    "| 4 | Format for DPO | ShareGPT-style JSON records |\n",
    "| 5 | Save JSON files | `train-v1.json`, `val-v1.json` |\n",
    "| 6 | Download extra data | PDF images + pre-built JSONs |\n",
    "| 7 | Unzip & collect paths | Extracted image paths |\n",
    "| 8 | Fix paths | Colab ‚Üí Kaggle path remapping |\n",
    "| 9 | Install dependencies | PyTorch, Transformers stack |\n",
    "| 10 | Install LlamaFactory | Training framework |\n",
    "| 11 | Register datasets | Custom entries in `dataset_info.json` |\n",
    "| 12 | Write YAML config | Training hyperparameters |\n",
    "| 13 | Run DPO training | Trained LoRA adapter weights |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31259,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
